{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import jieba\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans,DBSCAN\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"news_all_with_event\",\"rb\") as fp:\n",
    "    a = pickle.load(fp)\n",
    "\n",
    "a = a.reset_index(drop=True)\n",
    "a[\"org_index\"] = a.index\n",
    "contents = list(a[\"content\"])\n",
    "a[\"datetime\"] = pd.to_datetime(a[\"datetime\"])\n",
    "datetimes = list(a[\"datetime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "with open(\"valid_news_all_with_event\",\"rb\") as fp:\n",
    "    a = pickle.load(fp)\n",
    "\n",
    "a = a.reset_index(drop=True)\n",
    "a[\"org_index\"] = a.index\n",
    "contents = list(a[\"content\"])\n",
    "a[\"datetime\"] = pd.to_datetime(a[\"datetime\"])\n",
    "datetimes = list(a[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jieba Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 載入停用字檔案，並做成一個list\n",
    "stopwords_list = [line.strip() for line in open('stopwords.txt',\"r\").readlines()]\n",
    "\n",
    "### 載入同義字檔案，並做成一個字典\n",
    "syn_dict = {}\n",
    "with open(\"syn.txt\",\"r\") as f :\n",
    "    for line in f:\n",
    "        for word in line.strip(\"\\n\").split(\"\\t\")[1:]:\n",
    "            syn_dict[word] = line.strip(\"\\n\").split(\"\\t\")[0]\n",
    "\n",
    "### 載入中文斷詞字典\n",
    "jieba.set_dictionary(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwords(w):\n",
    "    if w not in stopwords_list:\n",
    "        return w\n",
    "\n",
    "def syn(w):\n",
    "    if w in syn_dict.keys():\n",
    "        w=syn_dict[w]\n",
    "        return w\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def cut(news):\n",
    "    w = jieba.cut(news, cut_all=False ,HMM=True)\n",
    "    return w\n",
    "\n",
    "def regular(w):\n",
    "    line = re.findall('[\\u4e00-\\u9fa5]+', w)\n",
    "    if len(line) > 0:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(paragraph):\n",
    "    me_words = []\n",
    "    words = cut(paragraph) \n",
    "    for w in words:\n",
    "        if w.startswith(\"一\"or\"三\"or\"四\"or\"五\"or\"六\"or\"七\"or\"八\"or\"九\"or\"十\") == False :\n",
    "            if len(w)>1:\n",
    "                w = regular(w)\n",
    "                if w is not None:\n",
    "                    w_stopwords = stopwords(w[0])\n",
    "                    if w_stopwords is not None:\n",
    "                        w_syn = syn(w_stopwords)\n",
    "                        me_words.append(w_syn)\n",
    "    return \" \".join(me_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaned_news(news_list):\n",
    "    cleaned_news_list = []\n",
    "    for news in copy.deepcopy(news_list):\n",
    "        if 'message' in news:\n",
    "            news[\"message\"] = text_cleaning(news[\"message\"])\n",
    "            cleaned_news_list.append(news)\n",
    "    return cleaned_news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noun_cleaned(content):   \n",
    "    words = [(word, flag) for word, flag in pseg.cut(content)]\n",
    "    noun_list = [w[0] for w in words if w[1]==\"n\"]\n",
    "    me_words=[]\n",
    "    for w in noun_list:\n",
    "        if w.startswith(\"一\"or\"三\"or\"四\"or\"五\"or\"六\"or\"七\"or\"八\"or\"九\"or\"十\") == False :\n",
    "            if len(w)>1:\n",
    "                w = regular(w)\n",
    "                if w is not None:\n",
    "                    w_stopwords = stopwords(w[0])\n",
    "                    if w_stopwords is not None:\n",
    "                        w_syn = syn(w_stopwords)\n",
    "                        me_words.append(w_syn)\n",
    "    return \" \".join(me_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Important Word in every Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_important_noun=[]\n",
    "for content in contents:\n",
    "    words = [(word, flag) for word, flag in pseg.cut(content)]\n",
    "    noun_list = [w[0] for w in words if w[1]==\"n\"]\n",
    "    me_words=[]\n",
    "    for w in noun_list:\n",
    "        if w.startswith(\"一\"or\"三\"or\"四\"or\"五\"or\"六\"or\"七\"or\"八\"or\"九\"or\"十\") == False :\n",
    "            if len(w)>1:\n",
    "                w = regular(w)\n",
    "                if w is not None:\n",
    "                    w_stopwords = stopwords(w[0])\n",
    "                    if w_stopwords is not None:\n",
    "                        w_syn = syn(w_stopwords)\n",
    "                        me_words.append(w_syn)\n",
    "    news_important_noun.append(\" \".join(me_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('news_all_important_noun',\"rb\") as f:\n",
    "    # pickle.dump(news_important_noun,f)\n",
    "    news_important_noun = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text  import  CountVectorizer  \n",
    "\n",
    "vectorizer = CountVectorizer()  \n",
    "X = vectorizer.fit_transform(news_important_noun)  \n",
    "words = vectorizer.get_feature_names()  \n",
    "\n",
    "from  sklearn.feature_extraction.text  import  TfidfTransformer  \n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(X)  \n",
    "# print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN process TFIDF Matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1st DBSCAN\n",
    "db = DBSCAN(n_jobs=-1,eps=0.7, min_samples=3,metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.fit(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認各個cluster有多少資料\n",
    "a[\"tag\"] = pd.Series(db.labels_)\n",
    "labels = list(set(db.labels_))\n",
    "sum = 0\n",
    "for i in set(labels):\n",
    "    findtag = (a[\"tag\"] == labels[i])\n",
    "    sum += len(a[findtag])\n",
    "    print(len(a[findtag]))\n",
    "print(sum)\n",
    "# 確認有幾個cluster\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(db.labels_))\n",
    "a[\"tag\"] = pd.Series(db.labels_)\n",
    "a[\"subtag\"] = -1\n",
    "for i in [ x for x in labels if x != -1]:\n",
    "    findtag = (a[\"tag\"] == labels[i])\n",
    "    cluster_pd = pd.DataFrame(a[findtag])\n",
    "    cluster_time_list = []\n",
    "    for index, row in cluster_pd.iterrows():\n",
    "         cluster_time_list.append([(row['datetime']-\n",
    "                                  list(cluster_pd['datetime'])[0]).days])\n",
    "\n",
    "    cluster_array = np.array(cluster_time_list)\n",
    "    # 可以看每一個相對時間\n",
    "#     cluster_time_list.sort()\n",
    "    print(cluster_time_list)\n",
    "    # # -----------------2nd DBSCAN----------------------------------------\n",
    "    cluster_db = DBSCAN(n_jobs=-1,eps=3, min_samples=2)\n",
    "    cluster_db.fit(cluster_array)\n",
    "    sub_labels = list(cluster_db.labels_)\n",
    "    count = 0\n",
    "    for index, row in cluster_pd.iterrows():\n",
    "        a.iloc[index,14] = sub_labels[count]\n",
    "        count += 1\n",
    "    print(\"insert\"+str(i)+\"cluster subtag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"news_with_event_eps0.7_with_tags_3days\",\"wb\")as f:\n",
    "    pickle.dump(a,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
